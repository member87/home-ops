---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-alerting
  namespace: grafana
data:
  contactpoints.yaml: |
    apiVersion: 1
    contactPoints:
      - orgId: 1
        name: discord
        receivers:
          - uid: discord-webhook
            type: discord
            settings:
              url: $DISCORD_WEBHOOK_URL
              title: "{{ .CommonLabels.alertname }}"
              message: |
                **Status:** {{ .Status }}
                **Summary:** {{ .CommonAnnotations.summary }}
                
                **Details:**
                {{ range .Alerts }}
                - **{{ .Labels.alertname }}**: {{ .Annotations.description }}
                  Severity: {{ .Labels.severity }}
                {{ end }}
              use_discord_username: true
            disableResolveMessage: false
  
  policies.yaml: |
    apiVersion: 1
    policies:
      - orgId: 1
        receiver: discord
        group_by: ['alertname', 'cluster', 'namespace']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 4h
        routes:
          - receiver: discord
            matchers:
              - severity =~ "critical|warning"
            continue: false
            group_wait: 10s
            group_interval: 5m
            repeat_interval: 12h
  
  rules.yaml: |
    apiVersion: 1
    groups:
      - orgId: 1
        name: kubernetes-alerts
        folder: Kubernetes
        interval: 1m
        rules:
          - uid: pod-not-ready
            title: Pod Not Ready
            condition: C
            data:
              - refId: A
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: prometheus
                model:
                  expr: kube_pod_status_phase{phase!="Running",phase!="Succeeded",phase!="Unknown"} * on(namespace,pod) group_left(owner_kind) kube_pod_owner{owner_kind!="Job"} > 0
                  refId: A
              - refId: B
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  conditions:
                    - evaluator:
                        params:
                          - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                          - A
                      reducer:
                        params: []
                        type: last
                      type: query
                  datasource:
                    type: __expr__
                    uid: __expr__
                  expression: A
                  refId: B
                  type: reduce
                  reducer: last
              - refId: C
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  conditions:
                    - evaluator:
                        params:
                          - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                          - C
                      reducer:
                        params: []
                        type: last
                      type: query
                  datasource:
                    type: __expr__
                    uid: __expr__
                  expression: B
                  refId: C
                  type: threshold
            noDataState: OK
            execErrState: OK
            for: 5m
            annotations:
              summary: Pod {{ $labels.namespace }}/{{ $labels.pod }} is not ready
              description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in {{ $labels.phase }} phase for more than 5 minutes. This alert excludes completed Jobs and Unknown phase pods.
            labels:
              severity: warning

          - uid: network-errors-high
            title: Network Errors High
            condition: C
            data:
              - refId: A
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: prometheus
                model:
                  expr: rate(node_network_receive_errs_total[5m]) + rate(node_network_transmit_errs_total[5m]) > 0.01
                  refId: A
              - refId: B
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: A
                  refId: B
                  type: reduce
                  reducer: last
              - refId: C
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: B
                  refId: C
                  type: threshold
                  conditions:
                    - evaluator:
                        params:
                          - 0.01
                        type: gt
            noDataState: OK
            execErrState: OK
            for: 5m
            annotations:
              summary: Network errors detected on {{ $labels.instance }}
              description: Interface {{ $labels.device }} on node {{ $labels.instance }} is experiencing network errors (current rate {{ $value }}/s)
            labels:
              severity: warning

          - uid: network-packet-drop-high
            title: Network Packet Drops High
            condition: C
            data:
              - refId: A
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: prometheus
                model:
                  expr: rate(node_network_receive_drop_total[5m]) + rate(node_network_transmit_drop_total[5m]) > 0.1
                  refId: A
              - refId: B
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: A
                  refId: B
                  type: reduce
                  reducer: last
              - refId: C
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: B
                  refId: C
                  type: threshold
                  conditions:
                    - evaluator:
                        params:
                          - 0.1
                        type: gt
            noDataState: OK
            execErrState: OK
            for: 5m
            annotations:
              summary: High packet drops on {{ $labels.instance }}
              description: Interface {{ $labels.device }} on node {{ $labels.instance }} is dropping packets (current rate {{ $value }}/s). This may indicate network congestion or hardware issues.
            labels:
              severity: warning

          - uid: tcp-retransmit-high
            title: TCP Retransmits High
            condition: C
            data:
              - refId: A
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: prometheus
                model:
                  expr: rate(node_netstat_Tcp_RetransSegs[5m]) / rate(node_netstat_Tcp_OutSegs[5m]) * 100 > 5
                  refId: A
              - refId: B
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: A
                  refId: B
                  type: reduce
                  reducer: last
              - refId: C
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: B
                  refId: C
                  type: threshold
                  conditions:
                    - evaluator:
                        params:
                          - 5
                        type: gt
            noDataState: OK
            execErrState: OK
            for: 5m
            annotations:
              summary: High TCP retransmit rate on {{ $labels.instance }}
              description: Node {{ $labels.instance }} has a TCP retransmit rate of {{ $value }}%, indicating network quality issues or packet loss
            labels:
              severity: warning

          - uid: pod-cpu-throttling
            title: Pod CPU Throttling
            condition: C
            data:
              - refId: A
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: prometheus
                model:
                  expr: rate(container_cpu_cfs_throttled_seconds_total[5m]) > 0.1
                  refId: A
              - refId: B
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: A
                  refId: B
                  type: reduce
                  reducer: last
              - refId: C
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: B
                  refId: C
                  type: threshold
                  conditions:
                    - evaluator:
                        params:
                          - 0.1
                        type: gt
            noDataState: OK
            execErrState: OK
            for: 10m
            annotations:
              summary: Pod {{ $labels.namespace }}/{{ $labels.pod }} experiencing CPU throttling
              description: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} is being CPU throttled. Consider increasing CPU limits.
            labels:
              severity: warning

          - uid: pod-memory-near-limit
            title: Pod Memory Near Limit
            condition: C
            data:
              - refId: A
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: prometheus
                model:
                  expr: (container_memory_working_set_bytes / container_spec_memory_limit_bytes) * 100 > 90
                  refId: A
              - refId: B
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: A
                  refId: B
                  type: reduce
                  reducer: last
              - refId: C
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: B
                  refId: C
                  type: threshold
                  conditions:
                    - evaluator:
                        params:
                          - 90
                        type: gt
            noDataState: OK
            execErrState: OK
            for: 5m
            annotations:
              summary: Pod {{ $labels.namespace }}/{{ $labels.pod }} memory usage near limit
              description: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} is using {{ $value }}% of its memory limit. Risk of OOM kill.
            labels:
              severity: warning

          - uid: pod-oom-killed
            title: Pod OOM Killed
            condition: C
            data:
              - refId: A
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: prometheus
                model:
                  expr: kube_pod_container_status_terminated_reason{reason="OOMKilled"} > 0
                  refId: A
              - refId: B
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: A
                  refId: B
                  type: reduce
                  reducer: last
              - refId: C
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: B
                  refId: C
                  type: threshold
                  conditions:
                    - evaluator:
                        params:
                          - 0
                        type: gt
            noDataState: OK
            execErrState: OK
            for: 1m
            annotations:
              summary: Pod {{ $labels.namespace }}/{{ $labels.pod }} was OOM killed
              description: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} was killed due to Out Of Memory. Increase memory limits.
            labels:
              severity: critical

          - uid: network-interface-down
            title: Network Interface Down
            condition: C
            data:
              - refId: A
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: prometheus
                model:
                  expr: node_network_up{device!~"veth.*|br.*|docker.*|virbr.*|lo"} == 0
                  refId: A
              - refId: B
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: A
                  refId: B
                  type: reduce
                  reducer: last
              - refId: C
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: B
                  refId: C
                  type: threshold
                  conditions:
                    - evaluator:
                        params:
                          - 0
                        type: eq
            noDataState: OK
            execErrState: OK
            for: 2m
            annotations:
              summary: Network interface {{ $labels.device }} is down on {{ $labels.instance }}
              description: Physical network interface {{ $labels.device }} on node {{ $labels.instance }} is down. Check network connectivity.
            labels:
              severity: critical

          - uid: persistent-volume-full
            title: Persistent Volume Nearly Full
            condition: C
            data:
              - refId: A
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: prometheus
                model:
                  expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100 > 85
                  refId: A
              - refId: B
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: A
                  refId: B
                  type: reduce
                  reducer: last
              - refId: C
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: B
                  refId: C
                  type: threshold
                  conditions:
                    - evaluator:
                        params:
                          - 85
                        type: gt
            noDataState: OK
            execErrState: OK
            for: 5m
            annotations:
              summary: PersistentVolume {{ $labels.persistentvolumeclaim }} is nearly full
              description: PersistentVolume {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is {{ $value }}% full
            labels:
              severity: warning

          - uid: node-memory-high
            title: Node Memory Usage High
            condition: C
            data:
              - refId: A
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: prometheus
                model:
                  expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
                  refId: A
              - refId: B
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: A
                  refId: B
                  type: reduce
                  reducer: last
              - refId: C
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: B
                  refId: C
                  type: threshold
                  conditions:
                    - evaluator:
                        params:
                          - 85
                        type: gt
            noDataState: OK
            execErrState: OK
            for: 5m
            annotations:
              summary: Node {{ $labels.instance }} memory usage is high
              description: Node {{ $labels.instance }} memory usage is above 85% (current value {{ $value }}%)
            labels:
              severity: warning

          - uid: node-cpu-high
            title: Node CPU Usage High
            condition: C
            data:
              - refId: A
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: prometheus
                model:
                  expr: 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
                  refId: A
              - refId: B
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: A
                  refId: B
                  type: reduce
                  reducer: last
              - refId: C
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: B
                  refId: C
                  type: threshold
                  conditions:
                    - evaluator:
                        params:
                          - 80
                        type: gt
            noDataState: OK
            execErrState: OK
            for: 5m
            annotations:
              summary: Node {{ $labels.instance }} CPU usage is high
              description: Node {{ $labels.instance }} CPU usage is above 80% (current value {{ $value }}%)
            labels:
              severity: warning

          - uid: node-disk-high
            title: Node Disk Usage High
            condition: C
            data:
              - refId: A
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: prometheus
                model:
                  expr: 100 - ((node_filesystem_avail_bytes{mountpoint="/var",fstype!="tmpfs"} / node_filesystem_size_bytes{mountpoint="/var",fstype!="tmpfs"}) * 100) > 80
                  refId: A
              - refId: B
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: A
                  refId: B
                  type: reduce
                  reducer: last
              - refId: C
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: B
                  refId: C
                  type: threshold
                  conditions:
                    - evaluator:
                        params:
                          - 80
                        type: gt
            noDataState: OK
            execErrState: OK
            for: 5m
            annotations:
              summary: Node {{ $labels.instance }} disk usage is high
              description: Node {{ $labels.instance }} disk usage on /var is above 80% (current value {{ $value }}%)
            labels:
              severity: warning

          - uid: pod-restart-high
            title: Pod Restarting Frequently
            condition: C
            data:
              - refId: A
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: prometheus
                model:
                  expr: rate(kube_pod_container_status_restarts_total[15m]) * 3600 > 5
                  refId: A
              - refId: B
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: A
                  refId: B
                  type: reduce
                  reducer: last
              - refId: C
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: B
                  refId: C
                  type: threshold
                  conditions:
                    - evaluator:
                        params:
                          - 5
                        type: gt
            noDataState: OK
            execErrState: OK
            for: 5m
            annotations:
              summary: Pod {{ $labels.namespace }}/{{ $labels.pod }} is restarting frequently
              description: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} is restarting more than 5 times per hour
            labels:
              severity: critical

          - uid: node-temp-high
            title: Node Temperature High
            condition: C
            data:
              - refId: A
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: prometheus
                model:
                  expr: node_hwmon_temp_celsius > 75
                  refId: A
              - refId: B
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: A
                  refId: B
                  type: reduce
                  reducer: last
              - refId: C
                relativeTimeRange:
                  from: 600
                  to: 0
                datasourceUid: __expr__
                model:
                  expression: B
                  refId: C
                  type: threshold
                  conditions:
                    - evaluator:
                        params:
                          - 75
                        type: gt
            noDataState: OK
            execErrState: OK
            for: 5m
            annotations:
              summary: Node {{ $labels.instance }} temperature is high
              description: Temperature sensor {{ $labels.sensor }} on node {{ $labels.instance }} is above 75°C (current value {{ $value }}°C)
            labels:
              severity: warning
